{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7663250,"sourceType":"datasetVersion","datasetId":4468671},{"sourceId":94020,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":78803,"modelId":103305}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch #GPUs: NVIDIA Tesla T4 x2\nif torch.cuda.is_available():\n    device1 = torch.device(\"cuda:0\")\n    print(f\"GPU found: {torch.cuda.get_device_name(0)}\")\n    properties1 = torch.cuda.get_device_properties(device1)\n    no_cudacores1 = properties1.multi_processor_count *64\n    print(f\"{no_cudacores1} CUDA cores on GPU\")\n    device2 = torch.device(\"cuda:1\")\n    print(f\"GPU found: {torch.cuda.get_device_name(1)}\")\n    properties2 = torch.cuda.get_device_properties(device2)\n    no_cudacores2 = properties2.multi_processor_count *64\n    print(f\"{no_cudacores2} CUDA cores on GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"No GPU found! using CPU\")","metadata":{"_uuid":"5e5565d0-0a9c-41d3-8a0e-6333d7c83ef3","_cell_guid":"eaaf7e80-923b-4d3b-bb45-a377ec6ac4d8","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\ntry:#libraries\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import optim as optim\n    from torch.optim import Adam\nexcept Exception as e:\n    !pip install optim\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import optim as optim\n    from torch.optim import Adam","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\ntry:#nuscenes-devkit\n    %matplotlib inline\n    from nuscenes.nuscenes import NuScenes\nexcept Exception as e:\n    !pip install nuscenes-devkit\n    %matplotlib inline\n    from nuscenes.nuscenes import NuScenes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom nuscenes.utils.data_classes import LidarPointCloud\n#for each file/instance of observation\ndirectory = '/kaggle/input/multislow-poli/CAVALIER_MULTI-SLOW-POLI/sweeps/LIDAR_FUSED'\nsave = r'/kaggle/working/'\nfiles = sorted(os.listdir(directory))\nprint(files[0])\npcdpath = os.path.join(directory, files[700])\npcd = LidarPointCloud.from_file(pcdpath)\n#print(pcd.points)\n#extract feature coordiantes from pcd\npcd4d = torch.tensor(pcd.points[:,:], dtype=torch.float32)\n#print(pcd4d)\nin_neurons = pcd4d.shape[1]\nprint(in_neurons)\n#moving to gpu here\npcd4d_gpu = pcd4d.to('cuda:0')\nprint(pcd4d_gpu)\n#extract lidar pcd\ndef extract_pcd(directory, ifile):\n    pcdpath = os.path.join(directory, ifile)\n    pcd = LidarPointCloud.from_file(pcdpath)\n    pcd4d = torch.tensor(pcd.points[:,:], dtype=torch.float32)\n    return pcd4d","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BEV map creation on GPU for one scan\ndef convert_scan_bevmap(pcd4d, resolution, bev_map_size, gpu):#resolution is also voxwel size\n    pcd4d_gpu = pcd4d.to(gpu)\n    threshold = 0\n    #extract x,y,z\n    x = pcd4d_gpu[0,:]\n    y = pcd4d_gpu[1,:]\n    z = pcd4d_gpu[2,:]\n    #print(x)\n    #initialize an empty BEV map\n    bev_map = torch.zeros((bev_map_size, bev_map_size), device=gpu)\n    #convert point cloud to BEV map\n    #Calculate the corresponding pixel for each point in the point cloud\n    pixelx = ((x / resolution) + bev_map_size / 2).floor().long() #shifting origins\n    pixely = ((y / resolution) + bev_map_size / 2).floor().long()\n    #Ignore points that fall outside the BEV map\n    mask = (pixelx >= 0) & (pixelx < bev_map_size) & (pixely >= 0) & (pixely < bev_map_size)\n    #Update the BEV map (using max pooling here, but can also use average pooling or other methods)\n    bev_map[pixelx[mask], pixely[mask]] = torch.max(bev_map[pixelx[mask], pixely[mask]], z[mask])\n    bev_map = torch.where(bev_map > threshold, torch.tensor(1.0, device=bev_map.device), torch.tensor(0.0, device=bev_map.device))\n    voxels = torch.stack((pixelx[mask], pixely[mask], z[mask]), dim=1).cpu().numpy()\n    return bev_map, voxels#returning binary bev map\n\nbevmap, voxels = convert_scan_bevmap(pcd4d, 0.2, 1000, device1)\nprint(bevmap.shape)\n# Plot the BEV map\nplt.imshow(bevmap.cpu(), cmap='hot')\nplt.show()\nprint(bevmap.max())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\n\n# BEV map creation on GPU for one scan with 3D voxelization\ndef convert_scan_bevmap(pcd4d, resolution, bev_map_size, depth_size, gpu):\n    \"\"\"\n    Converts a 3D point cloud to a 2D BEV map with 3D voxelization.\n    \n    Args:\n    - pcd4d (torch.Tensor): The input 3D point cloud data (shape: 4 x N).\n    - resolution (float): The voxel size or resolution.\n    - bev_map_size (int): Width and height of the BEV map in pixels.\n    - depth_size (int): Number of layers in the z-axis for voxelization.\n    - gpu (torch.device): Device to perform computation on (e.g., torch.device('cuda')).\n    \n    Returns:\n    - bev_map (torch.Tensor): The binary BEV map (2D tensor).\n    - voxels (numpy.ndarray): The voxelized coordinates.\n    \"\"\"\n    \n    pcd4d_gpu = pcd4d.to(gpu)\n    threshold = 0\n\n    # Extract x, y, z\n    x = pcd4d_gpu[0, :]\n    y = pcd4d_gpu[1, :]\n    z = pcd4d_gpu[2, :]\n\n    # Initialize an empty 3D voxel grid\n    voxel_grid = torch.zeros((bev_map_size, bev_map_size, depth_size), device=gpu)\n\n    # Calculate voxel indices for each point in the point cloud\n    voxel_x = ((x / resolution) + bev_map_size / 2).floor().long()\n    voxel_y = ((y / resolution) + bev_map_size / 2).floor().long()\n    voxel_z = ((z / resolution) + depth_size / 2).floor().long()\n\n    # Ignore points that fall outside the voxel grid\n    mask = (\n        (voxel_x >= 0) & (voxel_x < bev_map_size) &\n        (voxel_y >= 0) & (voxel_y < bev_map_size) &\n        (voxel_z >= 0) & (voxel_z < depth_size)\n    )\n\n    # Max pooling along the z-axis for each voxel\n    voxel_grid[voxel_x[mask], voxel_y[mask], voxel_z[mask]] = torch.max(\n        voxel_grid[voxel_x[mask], voxel_y[mask], voxel_z[mask]], z[mask]\n    )\n\n    # Project the 3D voxel grid to a 2D BEV map by taking the maximum along the z-axis\n    bev_map = torch.max(voxel_grid, dim=2)[0]\n\n    # Apply binary threshold to the BEV map\n    bev_map = torch.where(bev_map > threshold, torch.tensor(1.0, device=bev_map.device), torch.tensor(0.0, device=bev_map.device))\n\n    # Extract voxel coordinates to return as a numpy array for further visualization if needed\n    voxel_coords = torch.stack((voxel_x[mask], voxel_y[mask], voxel_z[mask]), dim=1).cpu().numpy()\n\n    return bev_map, voxel_coords  # Returning binary BEV map and voxel coordinates\n\n# Example usage\nbev_map_size = 1000  # BEV map size (width and height)\ndepth_size = 50      # Number of layers along the z-axis for voxelization\nresolution = 0.2     # Voxel resolution\n\nbevmap, voxels = convert_scan_bevmap(pcd4d, resolution, bev_map_size, depth_size, device1)\nprint(voxels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\ndef show_voxels(voxels, elev, azim, aspect_ratio):\n    fig = plt.figure(figsize=(10,7))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    # Extract x, y, z coordinates\n    x = voxels[:, 0]\n    y = voxels[:, 1]\n    z = voxels[:, 2]\n    \n    # Plot the voxels in 3D\n    ax.scatter(y, x, z, c=z**2, cmap='hot', marker='o', s=10)  # Adjust 's' for marker size\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.view_init(elev, azim)\n    ax.set_box_aspect(aspect_ratio)\n    plt.show()\nshow_voxels(voxels, -60, 90, (1,1.5,0.5))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class pyramidnetwork(nn.Module):\n    def __init__(self):\n        super(pyramidnetwork, self).__init__()\n        ##### encoder\n        self.conv1 = nn.Conv2d(16,32,1,1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv1_1 = nn.Conv2d(32,32,3,1,1)\n        self.bn1_1 = nn.BatchNorm2d(32)\n        self.poolconv1 = nn.Conv2d(32,32,3,1,1)\n        self.poolbn1 = nn.BatchNorm2d(32)\n        \n        self.conv2 = nn.Conv2d(32,64,3,1,1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv2_1 = nn.Conv2d(64,64,3,1,1)\n        self.bn2_1 = nn.BatchNorm2d(64)\n        self.poolconv2 = nn.Conv2d(64,64,3,1,1)\n        self.poolbn2 = nn.BatchNorm2d(64)\n        \n        self.conv3 = nn.Conv2d(64,128,3,1,1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv3_1 = nn.Conv2d(128,128,3,1,1)\n        self.bn3_1 = nn.BatchNorm2d(128)\n        self.poolconv3 = nn.Conv2d(128,128,3,1,1)\n        self.poolbn3 = nn.BatchNorm2d(128)\n        \n        self.conv4 = nn.Conv2d(128,256,3,1,1)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.conv4_1 = nn.Conv2d(256,256,3,1,1)\n        self.bn4_1 = nn.BatchNorm2d(256)\n        self.poolconv4 = nn.Conv2d(256,256,3,1,1)\n        self.poolbn4 = nn.BatchNorm2d(256)\n        \n        self.conv5 = nn.Conv2d(256,512,3,1,1)\n        self.bn5 = nn.BatchNorm2d(512)\n        self.conv5_1 = nn.Conv2d(512,512,3,1,1)\n        self.bn5_1 = nn.BatchNorm2d(512)\n        self.poolconv5 = nn.Conv2d(512,512,3,1,1)\n        self.poolbn5 = nn.BatchNorm2d(512)\n        \n        self.conv6 = nn.Conv2d(512,1024,3,1,1)\n        self.bn6 = nn.BatchNorm2d(1024)\n        self.conv6_1 = nn.Conv2d(1024,1024,3,1,1)\n        self.bn6_1 = nn.BatchNorm2d(1024)\n        self.poolconv6 = nn.Conv2d(1024,1024,3,1,1)\n        self.poolbn6 = nn.BatchNorm2d(1024)\n        ##### decoder\n        self.tranconv6 = nn.ConvTranspose2d(1024,512,3,2)\n        self.tranbn6 = nn.BatchNorm2d(512)\n        self.upconv6 = nn.ConvTranspose2d(1024,512,3,1,1)\n        self.upbn6 = nn.BatchNorm2d(512)\n        self.upconv6_1 = nn.ConvTranspose2d(512,512,3,1,1)\n        self.upbn6_1 = nn.BatchNorm2d(512)\n        \n        self.tranconv5 = nn.ConvTranspose2d(512,256,2,2)\n        self.tranbn5 = nn.BatchNorm2d(256)\n        self.upconv5 = nn.ConvTranspose2d(512,256,3,1,1)\n        self.upbn5 = nn.BatchNorm2d(256)\n        self.upconv5_1 = nn.ConvTranspose2d(256,256,3,1,1)\n        self.upbn5_1 = nn.BatchNorm2d(256)\n        \n        self.tranconv4 = nn.ConvTranspose2d(256,128,3,2)\n        self.tranbn4 = nn.BatchNorm2d(128)\n        self.upconv4 = nn.ConvTranspose2d(256,128,3,1,1)\n        self.upbn4 = nn.BatchNorm2d(128)\n        self.upconv4_1 = nn.ConvTranspose2d(128,128,3,1,1)\n        self.upbn4_1 = nn.BatchNorm2d(128)\n        \n        self.tranconv3 = nn.ConvTranspose2d(128,64,2,2)\n        self.tranbn3 = nn.BatchNorm2d(64)\n        self.upconv3 = nn.ConvTranspose2d(128,64,3,1,1)\n        self.upbn3 = nn.BatchNorm2d(64)\n        self.upconv3_1 = nn.ConvTranspose2d(64,64,3,1,1)\n        self.upbn3_1 = nn.BatchNorm2d(64)\n        \n        self.tranconv2 = nn.ConvTranspose2d(64,32,2,2)\n        self.tranbn2 = nn.BatchNorm2d(32)\n        self.upconv2 = nn.ConvTranspose2d(64,32,3,1,1)\n        self.upbn2 = nn.BatchNorm2d(32)\n        self.upconv2_1 = nn.ConvTranspose2d(32,32,3,1,1)\n        self.upbn2_1 = nn.BatchNorm2d(32)\n        \n        self.tranconv1 = nn.ConvTranspose2d(32,16,2,2)\n        self.tranbn1 = nn.BatchNorm2d(16)\n        self.upconv1 = nn.ConvTranspose2d(32,16,1,1)\n        self.upbn1 = nn.BatchNorm2d(16)\n        self.upconv1_1 = nn.ConvTranspose2d(16,1,3,1,1)\n        self.upbn1_1 = nn.BatchNorm2d(1)\n        \n        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n    def forward(self, pastscans,currscan):\n        #stack scans\n        xbevs = pastscans + [currscan]\n        xbevs_stack = torch.stack(xbevs)\n        xbevs_stack = xbevs_stack.float()\n        xbevs_ten = xbevs_stack.unsqueeze(0)\n        #print(xbevs_ten.shape)\n        #encoding\n        x1 = F.leaky_relu(self.bn1(self.conv1(xbevs_ten)),negative_slope=0.01)\n        x1 = F.leaky_relu(self.bn1_1(self.conv1_1(x1)),negative_slope=0.01)\n        x_1 = self.maxpool(x1)\n        x_1 = F.leaky_relu(self.poolbn1(self.poolconv1(x_1)),negative_slope=0.01)\n        x1 = self.maxpool(x1)\n\n        x2 = F.leaky_relu(self.bn2(self.conv2(x1)),negative_slope=0.01)\n        x2 = F.leaky_relu(self.bn2_1(self.conv2_1(x2)),negative_slope=0.01)\n        x_2 = self.maxpool(x2)\n        x_2 = F.leaky_relu(self.poolbn2(self.poolconv2(x_2)),negative_slope=0.01)\n        x2 = self.maxpool(x2)\n\n        x3 = F.leaky_relu(self.bn3(self.conv3(x2)),negative_slope=0.01)\n        x3 = F.leaky_relu(self.bn3_1(self.conv3_1(x3)),negative_slope=0.01)\n        x_3 = self.maxpool(x3)\n        x_3 = F.leaky_relu(self.poolbn3(self.poolconv3(x_3)),negative_slope=0.01)\n        x3 = self.maxpool(x3)\n        \n        x4 = F.leaky_relu(self.bn4(self.conv4(x3)),negative_slope=0.01)\n        x4 = F.leaky_relu(self.bn4_1(self.conv4_1(x4)),negative_slope=0.01)\n        x_4 = self.maxpool(x4)\n        x_4 = F.leaky_relu(self.poolbn4(self.poolconv4(x_4)),negative_slope=0.01)\n        x4 = self.maxpool(x4)\n\n        x5 = F.leaky_relu(self.bn5(self.conv5(x4)),negative_slope=0.01)\n        x5 = F.leaky_relu(self.bn5_1(self.conv5_1(x5)),negative_slope=0.01)\n        x_5 = self.maxpool(x5)\n        x_5 = F.leaky_relu(self.poolbn5(self.poolconv5(x_5)),negative_slope=0.01)\n        x5 = self.maxpool(x5)\n        \n        x6 = F.leaky_relu(self.bn6(self.conv6(x5)),negative_slope=0.01)\n        x6 = F.leaky_relu(self.bn6_1(self.conv6_1(x6)),negative_slope=0.01)\n        x_6 = self.maxpool(x6)\n        x_6 = F.leaky_relu(self.poolbn6(self.poolconv6(x_6)),negative_slope=0.01)\n        #decoding\n        x6up = F.leaky_relu(self.tranbn6(self.tranconv6(x_6)),negative_slope=0.01)\n        x6cat = torch.cat((x6up, x_5), dim=1)\n        x6de = F.leaky_relu(self.upbn6(self.upconv6(x6cat)),negative_slope=0.01)\n        x6de = F.leaky_relu(self.upbn6_1(self.upconv6_1(x6de)),negative_slope=0.01)\n        \n        x5up = F.leaky_relu(self.tranbn5(self.tranconv5(x_5)),negative_slope=0.01)\n        x5cat = torch.cat((x5up, x_4), dim=1)\n        x5de = F.leaky_relu(self.upbn5(self.upconv5(x5cat)),negative_slope=0.01)\n        x5de = F.leaky_relu(self.upbn5_1(self.upconv5_1(x5de)),negative_slope=0.01)\n\n        x4up = F.leaky_relu(self.tranbn4(self.tranconv4(x_4)),negative_slope=0.01)\n        x4cat = torch.cat((x4up, x_3), dim=1)\n        x4de = F.leaky_relu(self.upbn4(self.upconv4(x4cat)),negative_slope=0.01)\n        x4de = F.leaky_relu(self.upbn4_1(self.upconv4_1(x4de)),negative_slope=0.01)\n        \n        x3up = F.leaky_relu(self.tranbn3(self.tranconv3(x_3)),negative_slope=0.01)\n        x3cat = torch.cat((x3up, x_2), dim=1)\n        x3de = F.leaky_relu(self.upbn3(self.upconv3(x3cat)),negative_slope=0.01)\n        x3de = F.leaky_relu(self.upbn3_1(self.upconv3_1(x3de)),negative_slope=0.01)\n\n        x2up = F.leaky_relu(self.tranbn2(self.tranconv2(x_2)),negative_slope=0.01)\n        x2cat = torch.cat((x2up, x_1), dim=1)\n        x2de = F.leaky_relu(self.upbn2(self.upconv2(x2cat)),negative_slope=0.01)\n        x2de = F.leaky_relu(self.upbn2_1(self.upconv2_1(x2de)),negative_slope=0.01)\n\n        x1up = F.leaky_relu(self.tranbn1(self.tranconv1(x_1)),negative_slope=0.01)\n        x1cat = torch.cat((x1up, xbevs_ten), dim=1)\n        x1de = F.leaky_relu(self.upbn1(self.upconv1(x1cat)),negative_slope=0.01)\n        x1de = F.leaky_relu(self.upbn1_1(self.upconv1_1(x1de)),negative_slope=0.01)\n        print(x1de.size())\n        \n        return x1de","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(44)\nPPN = pyramidnetwork()\nPPN_gpu = PPN.to(device1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BEV map creation for a sequence of scans(6 scans=1 sequence)\nfor i in range(15, 16):#6=len(files)\n    #load prev scans(5 scans as prev scans)\n    previous_scans = []\n    for j in range(i-15, i):\n        print(j)\n        pcd4d = extract_pcd(directory, files[j])\n        bevmap = convert_scan_bevmap(pcd4d, 0.2, 1000, 50, device1)\n        # Plot the BEV map\n        #plt.imshow(bevmap.cpu(), cmap='hot')\n        #plt.show()\n        previous_scans.append(bevmap)\n    #load current scan\n    print(i,r\"(current)\")\n    pcd4d = extract_pcd(directory, files[i])\n    current_scan = convert_scan_bevmap(pcd4d, 0.2, 1000, 50, device1)\n    # Plot the BEV map\n    #plt.imshow(current_scan.cpu(), cmap='hot')\n    #plt.show()\n    \n    #feed into PPN\n    ppn_pred_gpu = PPN_gpu(previous_scans,current_scan)\n    ppn_pred = ppn_pred_gpu.cpu().detach().numpy()\n    ppn_pred_pl = ppn_pred[0, 0, :, :]#batch=1,channel=1\n    #plot the image\n    plt.imshow(ppn_pred_pl, cmap='hot')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"#benchmarking\n# Hyperparameters\nlearning_rate = 0.0001\nbeta = 1  # Smooth L1 loss parameter\nlossfun1 = nn.SmoothL1Loss()\nlossfun2 = nn.MSELoss()\n# Optimizer\noptimizer = Adam(PPN_gpu.parameters(), lr=learning_rate)\ntraining_losses = []\nvalidation_losses = []\nfor e in range(0,1):\n    for i in range(15,700):\n        previous_scans=[]\n        for j in range(i-15,i):\n            pcd4d = extract_pcd(directory, files[j])\n            bevmap = convert_scan_bevmap(pcd4d,0.2,1000, device1)\n            previous_scans.append(bevmap)\n        pcd4d = extract_pcd(directory, files[i])\n        current_scan = convert_scan_bevmap(pcd4d, 0.2,1000, device1)\n        #feed into PPN\n        ppn_pred_gpu = PPN_gpu(previous_scans,current_scan)\n        #corresponding future training scene evolution\n        future_scans = []\n        for k in range(i+2,i+2+16):\n            futpcd = extract_pcd(directory, files[k])\n            futbev = convert_scan_bevmap(futpcd, 0.2, 1000, device1)\n            future_scans.append(futbev)\n        bevs_stack = torch.stack(future_scans)\n        bevs_stack = bevs_stack.float()\n        bevs_ten = bevs_stack.unsqueeze(0)\n        #feed into ground turth generator\n        loss = lossfun1(ppn_pred_gpu, bevs_ten) + lossfun2(ppn_pred_gpu, bevs_ten)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        training_losses.append(loss.item())\n        print(i,\"loss:\",loss)\n        # Validation loop\n        with torch.no_grad():  # Ensure no gradients are computed\n            for vi in range(5000+i, 5001+i):  # files for validation\n                vprevious_scans = []\n                for vj in range(vi - 15, vi):\n                    vpcd4d = extract_pcd(directory, files[vj])\n                    vbevmap = convert_scan_bevmap(vpcd4d, 0.2, 1000, device1)\n                    vprevious_scans.append(vbevmap)\n                vpcd4d = extract_pcd(directory, files[vi])\n                vcurrent_scan = convert_scan_bevmap(vpcd4d, 0.2, 1000, device1)\n                # Feed into PPN\n                ppn_v_gpu = PPN_gpu(vprevious_scans, vcurrent_scan)\n                # Corresponding future validation scene evolution\n                vfuture_scans = []\n                for vk in range(vi + 2, vi + 2 + 16):\n                    vfutpcd = extract_pcd(directory, files[vk])\n                    vfutbev = convert_scan_bevmap(vfutpcd, 0.2, 1000, device1)\n                    vfuture_scans.append(vfutbev)\n                vbevs_stack = torch.stack(vfuture_scans)\n                vbevs_stack = vbevs_stack.float()\n                vbevs_ten = vbevs_stack.unsqueeze(0)\n                # Calculate validation loss\n                vloss = lossfun1(ppn_v_gpu, vbevs_ten) + lossfun2(ppn_v_gpu, vbevs_ten)\n            validation_losses.append(vloss.item()) \n            print(vi,\"Validation loss:\", vloss)\n\n# Learning curve after training\nplt.plot(training_losses, label=\"Training Loss\", color='blue')\nplt.plot(validation_losses, label=\"Validation Loss\", color='orange')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Learning Curve for benchmarking\")\nplt.legend()\nplt.show()\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#scene prediction\nfor i in range(7000,7001):\n    previous_scans=[]\n    for j in range(i-15,i):\n        pcd4d = extract_pcd(directory, files[j])\n        bevmap = convert_scan_bevmap(pcd4d,0.2,1000,device1)\n        previous_scans.append(bevmap)\n    pcd4d = extract_pcd(directory, files[i])\n    current_scan = convert_scan_bevmap(pcd4d, 0.2,1000,device1)\n    plt.imshow(current_scan.cpu(), cmap='hot')\n    plt.show()\n    #feed into PPN\n    ppn_pred_gpu = PPN_gpu(previous_scans,current_scan)\n    future_np = ppn_pred_gpu.cpu().detach().numpy()\n    future_pl = future_np[0, 0, :, :]#batch=1,channel=1\n    #plot the image\n    plt.imshow(future_pl, cmap='hot')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save trained model's parameters\ntorch.save(PPN_gpu.state_dict(), \"racetrained-PPN-segnet.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class autonetwork(nn.Module):\n    def __init__(self):\n        super(autonetwork, self).__init__()\n        ##### encoder\n        self.conv1 = nn.Conv2d(16,32,1,1)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.conv1_1 = nn.Conv2d(32,32,3,1,1)\n        self.bn1_1 = nn.BatchNorm2d(32)\n        \n        self.conv2 = nn.Conv2d(32,64,3,1,1)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.conv2_1 = nn.Conv2d(64,64,3,1,1)\n        self.bn2_1 = nn.BatchNorm2d(64)\n        \n        self.conv3 = nn.Conv2d(64,128,3,1,1)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.conv3_1 = nn.Conv2d(128,128,3,1,1)\n        self.bn3_1 = nn.BatchNorm2d(128)\n        \n        self.conv4 = nn.Conv2d(128,256,3,1,1)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.conv4_1 = nn.Conv2d(256,256,3,1,1)\n        self.bn4_1 = nn.BatchNorm2d(256)\n        \n        self.conv5 = nn.Conv2d(256,512,3,1,1)\n        self.bn5 = nn.BatchNorm2d(512)\n        self.conv5_1 = nn.Conv2d(512,512,3,1,1)\n        self.bn5_1 = nn.BatchNorm2d(512)\n        \n        self.conv6 = nn.Conv2d(512,1024,3,1,1)\n        self.bn6 = nn.BatchNorm2d(1024)\n        self.conv6_1 = nn.Conv2d(1024,1024,3,1,1)\n        self.bn6_1 = nn.BatchNorm2d(1024)\n        ##### decoder\n        self.tranconv6 = nn.ConvTranspose2d(1024,512,3,2)\n        self.tranbn6 = nn.BatchNorm2d(512)\n        self.upconv6_1 = nn.ConvTranspose2d(512,512,3,1,1)\n        self.upbn6_1 = nn.BatchNorm2d(512)\n        \n        self.tranconv5 = nn.ConvTranspose2d(512,256,2,2)\n        self.tranbn5 = nn.BatchNorm2d(256)\n        self.upconv5_1 = nn.ConvTranspose2d(256,256,3,1,1)\n        self.upbn5_1 = nn.BatchNorm2d(256)\n        \n        self.tranconv4 = nn.ConvTranspose2d(256,128,3,2)\n        self.tranbn4 = nn.BatchNorm2d(128)\n        self.upconv4_1 = nn.ConvTranspose2d(128,128,3,1,1)\n        self.upbn4_1 = nn.BatchNorm2d(128)\n        \n        self.tranconv3 = nn.ConvTranspose2d(128,64,2,2)\n        self.tranbn3 = nn.BatchNorm2d(64)\n        self.upconv3_1 = nn.ConvTranspose2d(64,64,3,1,1)\n        self.upbn3_1 = nn.BatchNorm2d(64)\n        \n        self.tranconv2 = nn.ConvTranspose2d(64,32,2,2)\n        self.tranbn2 = nn.BatchNorm2d(32)\n        self.upconv2_1 = nn.ConvTranspose2d(32,32,3,1,1)\n        self.upbn2_1 = nn.BatchNorm2d(32)\n        \n        self.tranconv1 = nn.ConvTranspose2d(32,16,2,2)\n        self.tranbn1 = nn.BatchNorm2d(16)\n        self.upconv1_1 = nn.ConvTranspose2d(16,1,3,1,1)\n        self.upbn1_1 = nn.BatchNorm2d(1)\n        \n        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n    def forward(self, pastscans,currscan):\n        #stack scans\n        xbevs = pastscans + [currscan]\n        xbevs_stack = torch.stack(xbevs)\n        xbevs_stack = xbevs_stack.float()\n        xbevs_ten = xbevs_stack.unsqueeze(0)\n        #print(xbevs_ten.shape)\n        #encoding\n        x1 = F.leaky_relu(self.bn1(self.conv1(xbevs_ten)),negative_slope=0.01)\n        x1 = F.leaky_relu(self.bn1_1(self.conv1_1(x1)),negative_slope=0.01)\n        x1 = self.maxpool(x1)\n\n        x2 = F.leaky_relu(self.bn2(self.conv2(x1)),negative_slope=0.01)\n        x2 = F.leaky_relu(self.bn2_1(self.conv2_1(x2)),negative_slope=0.01)\n        x2 = self.maxpool(x2)\n\n        x3 = F.leaky_relu(self.bn3(self.conv3(x2)),negative_slope=0.01)\n        x3 = F.leaky_relu(self.bn3_1(self.conv3_1(x3)),negative_slope=0.01)\n        x3 = self.maxpool(x3)\n        \n        x4 = F.leaky_relu(self.bn4(self.conv4(x3)),negative_slope=0.01)\n        x4 = F.leaky_relu(self.bn4_1(self.conv4_1(x4)),negative_slope=0.01)\n        x4 = self.maxpool(x4)\n\n        x5 = F.leaky_relu(self.bn5(self.conv5(x4)),negative_slope=0.01)\n        x5 = F.leaky_relu(self.bn5_1(self.conv5_1(x5)),negative_slope=0.01)\n        x5 = self.maxpool(x5)\n        \n        x6 = F.leaky_relu(self.bn6(self.conv6(x5)),negative_slope=0.01)\n        x6 = F.leaky_relu(self.bn6_1(self.conv6_1(x6)),negative_slope=0.01)\n        x_6 = self.maxpool(x6)\n        #decoding\n        x6up = F.leaky_relu(self.tranbn6(self.tranconv6(x_6)),negative_slope=0.01)\n        x6up = F.leaky_relu(self.upbn6_1(self.upconv6_1(x6up)),negative_slope=0.01)\n\n        x5up = F.leaky_relu(self.tranbn5(self.tranconv5(x6up)),negative_slope=0.01)\n        x5up = F.leaky_relu(self.upbn5_1(self.upconv5_1(x5up)),negative_slope=0.01)\n    \n        x4up = F.leaky_relu(self.tranbn4(self.tranconv4(x5up)),negative_slope=0.01)\n        x4up = F.leaky_relu(self.upbn4_1(self.upconv4_1(x4up)),negative_slope=0.01)\n\n        x3up = F.leaky_relu(self.tranbn3(self.tranconv3(x4up)),negative_slope=0.01)\n        x3up = F.leaky_relu(self.upbn3_1(self.upconv3_1(x3up)),negative_slope=0.01)\n\n        x2up = F.leaky_relu(self.tranbn2(self.tranconv2(x3up)),negative_slope=0.01)\n        x2up = F.leaky_relu(self.upbn2_1(self.upconv2_1(x2up)),negative_slope=0.01)\n\n        x1up = F.leaky_relu(self.tranbn1(self.tranconv1(x2up)),negative_slope=0.01)\n        x1up = F.leaky_relu(self.upbn1_1(self.upconv1_1(x1up)),negative_slope=0.01)\n        print(x1up.shape)\n        \n        return x1up","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(44)\naPPN = autonetwork()\naPPN_gpu = aPPN.to(device2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BEV map creation for a sequence of scans(6 scans=1 sequence)\nfor i in range(15, 16):#6=len(files)\n    #load prev scans(5 scans as prev scans)\n    previous_scans = []\n    for j in range(i-15, i):\n        print(j)\n        pcd4d = extract_pcd(directory, files[j])\n        bevmap = convert_scan_bevmap(pcd4d, 0.2, 1000, device2)\n        # Plot the BEV map\n        #plt.imshow(bevmap.cpu(), cmap='hot')\n        #plt.show()\n        previous_scans.append(bevmap)\n    #load current scan\n    print(i,r\"(current)\")\n    pcd4d = extract_pcd(directory, files[i])\n    current_scan = convert_scan_bevmap(pcd4d, 0.2, 1000, device2)\n    # Plot the BEV map\n    #plt.imshow(current_scan.cpu(), cmap='hot')\n    #plt.show()\n    \n    #feed into PPN\n    appn_pred_gpu = aPPN_gpu(previous_scans,current_scan)\n    appn_pred = appn_pred_gpu.cpu().detach().numpy()\n    appn_pred_pl = appn_pred[0, 0, :, :]#batch=1,channel=1\n    #plot the image\n    plt.imshow(appn_pred_pl, cmap='hot')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\n#intersrction over union as loss\ndef IoU_loss(pred, target):\n    # Squeeze the batch dimension (if present)\n    pred = pred.squeeze(0)\n    target = target.squeeze(0)\n    # Intersection\n    intersection = torch.sum(pred * target)\n    #Using logical OR for efficient union calculation\n    union = torch.sum(torch.logical_or(pred, target)) - intersection\n    # IoU loss (add epsilon for stability)\n    iou = torch.div(intersection + 1e-6, union + 1e-6)\n    loss = 1 - iou\n    return loss.unsqueeze(0)\n#canny edge detector with mse + smoothl1 as loss function\ndef MSSCE(pred, truth, th1,th2, gpu, al=0.85):\n    pred = pred.float()\n    truth = truth.float()\n    prednp = pred.cpu().detach().numpy()\n    truthnp = truth.cpu().detach().numpy()\n    #canny edge detectors\n    pred_edgenp = np.array([cv2.Canny(i.astype(np.uint8), th1, th2) for i in prednp])\n    truth_edgenp = np.array([cv2.Canny(i.astype(np.uint8), th1, th2) for i in truthnp])\n    pred_edge = torch.from_numpy(pred_edgenp).float().to(gpu)\n    truth_edge = torch.from_numpy(truth_edgenp).float().to(gpu)\n    mse = lossfun2(pred,truth)\n    edgeloss = lossfun2(pred_edge, truth_edge)\n    sl1 = lossfun1(pred,truth)\n    sedgeloss = lossfun1(pred_edge, truth_edge)\n    #MSCE loss\n    msceloss = al*mse+(1-al)*edgeloss + al*sl1+(1-al)*sedgeloss\n    return msceloss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#training MSSCE\n# Hyperparameters\nlearning_rate = 0.001\nearly_stopping_threshold = 0.1\nbeta = 1  # Smooth L1 loss parameter\nlossfun1 = nn.SmoothL1Loss()\nlossfun2 = nn.MSELoss()\n# Optimizer\noptimizer = Adam(aPPN_gpu.parameters(), lr=learning_rate)\ntraining_losses = []\nvalidation_losses = []\nfor e in range(0,1):\n    for i in range(15,700):\n        previous_scans1=[]\n        previous_scans2=[]\n        for j in range(i-15,i):\n            pcd4d = extract_pcd(directory, files[j])\n            bevmap2 = convert_scan_bevmap(pcd4d,0.2,1000, device2)\n            bevmap1 = convert_scan_bevmap(pcd4d,0.2,1000, device1)\n            previous_scans2.append(bevmap2)\n            previous_scans1.append(bevmap1)\n        pcd4d = extract_pcd(directory, files[i])\n        current_scan2 = convert_scan_bevmap(pcd4d, 0.2,1000, device2)\n        current_scan1 = convert_scan_bevmap(pcd4d, 0.2,1000, device1)\n        #feed into aPPN\n        appn_pred_gpu = aPPN_gpu(previous_scans2,current_scan2)\n        #corresponding segmented training scene evolution\n        ppn_pred_gpu = PPN_gpu(previous_scans1,current_scan1)\n        ppn_pred_gpu2 = ppn_pred_gpu.to(device2)\n        #feed into loss function\n        loss = MSSCE(appn_pred_gpu, ppn_pred_gpu2,0,0,device2)\n        if loss.item() < early_stopping_threshold:\n            print(\"Early stopping: Loss reached threshold (\", loss.item(), \")\")\n            break\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        training_losses.append(loss.item())\n        print(i,\"loss:\",loss)\n        # Validation loop\n        with torch.no_grad():  # Ensure no gradients are computed\n            for vi in range(5000+i, 5001+i):  # files for validation\n                vprevious_scans1 = []\n                vprevious_scans2 = []\n                for vj in range(vi - 15, vi):\n                    vpcd4d = extract_pcd(directory, files[vj])\n                    vbevmap2 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device2)\n                    vbevmap1 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device1)\n                    vprevious_scans2.append(vbevmap2)\n                    vprevious_scans1.append(vbevmap1)\n                vpcd4d = extract_pcd(directory, files[vi])\n                vcurrent_scan2 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device2)\n                vcurrent_scan1 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device1)\n                # Feed into aPPN\n                appn_v_gpu = aPPN_gpu(vprevious_scans2, vcurrent_scan2)\n                #corresponding segmented training scene evolution\n                ppn_v_gpu = PPN_gpu(vprevious_scans1,vcurrent_scan1)\n                ppn_v_gpu2 = ppn_v_gpu.to(device2)\n                # Calculate validation loss\n                vloss = MSSCE(appn_v_gpu, ppn_v_gpu2,0,0,device2)\n            validation_losses.append(vloss.item()) \n            print(vi,\"Validation loss:\", vloss)\n# learning curve after training\nimport matplotlib.pyplot as plt\nplt.plot(training_losses, label=\"Training Loss\", color='blue')\nplt.plot(validation_losses, label=\"Validation Loss\", color='orange')\nplt.xlabel(\"Training Iteration\")\nplt.ylabel(f\"MSSCE Loss \")\nplt.title(\"Learning Curve for Scene Evolution\")\nplt.legend()\nplt.show()","metadata":{"_kg_hide-output":false,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#scene prediction\nfor i in range(7100,7101):\n    previous_scans=[]\n    for j in range(i-15,i):\n        pcd4d = extract_pcd(directory, files[j])\n        bevmap = convert_scan_bevmap(pcd4d,0.2,1000,device2)\n        previous_scans.append(bevmap)\n    pcd4d = extract_pcd(directory, files[i])\n    current_scan = convert_scan_bevmap(pcd4d, 0.2,1000,device2)\n    print(current_scan.max())\n    plt.imshow(current_scan.cpu(), cmap='hot')\n    plt.show()\n    #feed into PPN\n    appn_pred_gpu = aPPN_gpu(previous_scans,current_scan)\n    #prediction loss\n    previous_scans=[]\n    for j in range(i-15,i):\n        pcd4d = extract_pcd(directory, files[j])\n        bevmap = convert_scan_bevmap(pcd4d,0.2,1000,device1)\n        previous_scans.append(bevmap)\n    pcd4d = extract_pcd(directory, files[i])\n    current_scan = convert_scan_bevmap(pcd4d, 0.2,1000,device1)\n    print(current_scan.max())\n    ppn_pred_gpu = PPN_gpu(previous_scans,current_scan)\n    ppn_pred_gpu2 = ppn_pred_gpu.to(device2)\n    afuture_np = appn_pred_gpu.cpu().detach().numpy()\n    afuture_pl = afuture_np[0, 0, :, :]#batch=1,channel=1\n    #plot the image\n    plt.imshow(afuture_pl, cmap='hot')\n    plt.show()\n    #mssce = MSSCE(appn_pred_gpu,ppn_pred_gpu2,0,0,device2)\n    #print(f\"MSSCE loss: {mssce}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"#training IOU\n# Hyperparameters\nlearning_rate = 0.0001\nearly_stopping_threshold = 0.01\n# Optimizer\noptimizer = Adam(aPPN_gpu.parameters(), lr=learning_rate)\ntraining_losses = []\nvalidation_losses = []\nfor e in range(0,1):\n    for i in range(15,700):\n        previous_scans1=[]\n        previous_scans2=[]\n        for j in range(i-15,i):\n            pcd4d = extract_pcd(directory, files[j])\n            bevmap2 = convert_scan_bevmap(pcd4d,0.2,1000, device2)\n            bevmap1 = convert_scan_bevmap(pcd4d,0.2,1000, device1)\n            previous_scans2.append(bevmap2)\n            previous_scans1.append(bevmap1)\n        pcd4d = extract_pcd(directory, files[i])\n        current_scan2 = convert_scan_bevmap(pcd4d, 0.2,1000, device2)\n        current_scan1 = convert_scan_bevmap(pcd4d, 0.2,1000, device1)\n        #feed into aPPN\n        appn_pred_gpu = aPPN_gpu(previous_scans2,current_scan2)\n        #corresponding segmented training scene evolution\n        ppn_pred_gpu = PPN_gpu(previous_scans1,current_scan1)\n        ppn_pred_gpu2 = ppn_pred_gpu.to(device2)\n        #feed into loss function\n        loss = IoU_loss(appn_pred_gpu, ppn_pred_gpu2)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        training_losses.append(loss.item())\n        print(i,\"loss:\",loss)\n        if loss.item() < early_stopping_threshold:\n            print(\"Early stopping: Loss reached threshold (\", loss.item(), \")\")\n            break\n        # Validation loop\n        with torch.no_grad():  # Ensure no gradients are computed\n            for vi in range(5000+i, 5001+i):  # files for validation\n                vprevious_scans1 = []\n                vprevious_scans2 = []\n                for vj in range(vi - 15, vi):\n                    vpcd4d = extract_pcd(directory, files[vj])\n                    vbevmap2 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device2)\n                    vbevmap1 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device1)\n                    vprevious_scans2.append(vbevmap2)\n                    vprevious_scans1.append(vbevmap1)\n                vpcd4d = extract_pcd(directory, files[vi])\n                vcurrent_scan2 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device2)\n                vcurrent_scan1 = convert_scan_bevmap(vpcd4d, 0.2, 1000, device1)\n                # Feed into aPPN\n                appn_v_gpu = aPPN_gpu(vprevious_scans2, vcurrent_scan2)\n                #corresponding segmented training scene evolution\n                ppn_v_gpu = PPN_gpu(vprevious_scans1,vcurrent_scan1)\n                ppn_v_gpu2 = ppn_v_gpu.to(device2)\n                # Calculate validation loss\n                vloss = IoU_loss(appn_v_gpu, ppn_v_gpu2)\n            validation_losses.append(vloss.item()) \n            print(vi,\"Validation loss:\", vloss)\n# learning curve after training\nimport matplotlib.pyplot as plt\nplt.plot(training_losses, label=\"Training Loss\", color='blue')\nplt.plot(validation_losses, label=\"Validation Loss\", color='orange')\nplt.xlabel(\"Training Iteration\")\nplt.ylabel(f\"IoU Loss \")\nplt.title(\"Learning Curve for Scene Evolution\")\nplt.legend()\nplt.show()\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# computation time on cpu+gpu\nimport time\nimport torch\ni=15\ntppn_gpu = autonetwork().to(device1)\nstart_time = time.perf_counter()\n# Run some things here\nprevious_scans=[]\nfor j in range(i-15,i):\n    pcd4d = extract_pcd(directory, files[j])\n    bevmap = convert_scan_bevmap(pcd4d,0.2,1000,device1)\n    previous_scans.append(bevmap)\npcd4d = extract_pcd(directory, files[i])\ncurrent_scan = convert_scan_bevmap(pcd4d, 0.2,1000,device1)\n#feed into PPN and aPPN sequentially\nppn_pred_gpu = PPN_gpu(previous_scans,current_scan)\nprevious_scans=[]\nfor j in range(i-15,i):\n    pcd4d = extract_pcd(directory, files[j])\n    bevmap = convert_scan_bevmap(pcd4d,0.2,1000,device1)\n    previous_scans.append(bevmap)\npcd4d = extract_pcd(directory, files[i])\ncurrent_scan = convert_scan_bevmap(pcd4d, 0.2,1000,device1)\ntppn_pred_gpu = tppn_gpu(previous_scans,current_scan)\n\nelapsed_time = (time.perf_counter() - start_time)\nprint(f\"Model inference time (might include non-GPU operations): {elapsed_time} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#model with parallel dual neural networks\nclass Model(nn.Module):\n    def __init__(self,PPN_gpu,aPPN_gpu):\n        super(Model,self).__init__()\n        self.pyrnet = PPN_gpu\n        self.autonet = aPPN_gpu\n    \n    def forward(self,directory,files,i,device1,device2):\n        previous_scans1 = []\n        previous_scans2 = []\n        for j in range(i-15, i):\n            print(j)\n            pcd4d = extract_pcd(directory, files[j])\n            bevmap1 = convert_scan_bevmap(pcd4d, 0.2, 1000, device1)\n            previous_scans1.append(bevmap1)\n            bevmap2 = convert_scan_bevmap(pcd4d, 0.2, 1000, device2)\n            previous_scans2.append(bevmap2)\n        #load current scan\n        print(i,r\"(current)\")\n        pcd4d = extract_pcd(directory, files[i])\n        current_scan1 = convert_scan_bevmap(pcd4d, 0.2, 1000, device1)\n        current_scan2 = convert_scan_bevmap(pcd4d, 0.2, 1000, device2)\n        \n        pyrnet_out = self.pyrnet(previous_scans1,current_scan1)\n        autonet_out = self.autonet(previous_scans2,current_scan2)\n        \n        return pyrnet_out, autonet_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(44)\nmodel = Model(PPN_gpu,aPPN_gpu)\nfor i in range(700,701):\n    pyrnet_pred_gpu, autonet_pred_gpu = model(directory, files, i, device1, device2)\n    pyr_pred = pyrnet_pred_gpu.cpu().detach().numpy()\n    pyr_pred_pl = pyr_pred[0, 0, :, :]\n    plt.imshow(pyr_pred_pl, cmap='hot')\n    plt.show()\n    auto_pred = autonet_pred_gpu.cpu().detach().numpy()\n    auto_pred_pl = auto_pred[0, 0, :, :]\n    plt.imshow(auto_pred_pl, cmap='hot')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# computation time on cpu+gpu\nimport time\nimport torch\ni=15\nstart_time = time.perf_counter()\npyrnet_pred_gpu, autonet_pred_gpu = model(directory, files, i, device1, device2)\n\nelapsed_time = (time.perf_counter() - start_time)\nprint(f\"Model inference time (might include non-GPU operations): {elapsed_time} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#save trained model's parameters\ntorch.save(aPPN_gpu.state_dict(), \"racetrained-PPN-recnet.pth\")\ntorch.save(model.state_dict(), \"racetrained-PPN-model.pth\")\ntorch.save(model.state_dict(), \"pytorch_model.bin\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}